---
layout: single
title:  "주간학습정리 Pytorch"
---
## 배운 내용을 필요할 때 찾아볼 수 있게 가이드북 형태로 작성


### Data 타입   
```python
  dtype = torch.int32 or torch.int 
  dtype = torch.float32 or torch.float
```

### 기초 문법

```python
torch.min(), max() # min, max, sum, prod, mean, var, std 다 지원함
dim() # 차원
size(), shape() # 크기
x.numel() # Tensor에 있는 요소의 총 개수 확인
```

### Tensor 생성

```python
torch.rand(n) # 특정 크기의 연속균등분포 난수
torch.randn_like(n) # n과 크기와 자료형이 같은 표준정규분포(평균이 0 표준편차가 1인 곡선) 난수
q = torch.empty(5) # 성능과 메모리 사용성의 이점있음
q.fill_(3.0) # inplace 방식
v = torch.from_numpy(u).float() # Numpy로 생성된 Tensor는 기본적으로 정수형
x = torch.IntTensor() # FloatTensor
y = x.clone() # Tensor 복제
z = x.detach() # clone 과 다른점: 계산그래프에서 분리하여 새로운 Tensor z에 저장
```

### device

```python
a.device # 현재 디바이스
torch.cuda.is_avilable() # CUDA를 사용할 수 있는 환경인지 확인
torch.cuda.device_count() # 사용 가능한 GPU 개수
torch.cuda.get_device_name(device=0) # cuda device name 확인
torch.tensor().to(device='cuda') or torch.tensor().cuda()
```

### indexing & slicing

indexing : Tensor의 특정위치의 요소에 접근하는 것 

slicing : 부분집합을 선택해 새로운 Sub Tensor 생성하는 과정


### Tensor 모양 변경

```python
연속적 메모리 할당의 이해
view() 와 reshape() 의 차이 이해
is_contiguous() # 메모리의 연속성 확인
flatten()  # 전처리를 위한 평탄화
transpose() # 특정 두 차원의 축을 뒤바꿈
squeeze() & unsqeeze() # 차원 축소 확장
stack() # Tensor들 간의 결합
```

### Tensor 산술연산
```python
a.add_(b) # inplace 방식
torch.sub(f,e) # mul, pow,
torch.eq(v,w) # 결과는 Boolean Tensor로 출력 # ne, ge, lt, le
logical_and(a,b)  # 논리곱(and) 논리합(or) 배타적 논리합(XOR) 이해
```

### 노름에대한 이해,활용 Tensor의 행렬곱
```python
# 1-D Tensor의 노름 이해
L1(맨해튼), L2(유클리드), L∞
torch.norm(a, p=2), norm(a, p=float('inf'))

# 맨해튼, 유클리드, 코사인 유사도
cosine_similarity = torch.dot(b,c) / (torch.norm(b, p=2)) * (torch.norm(c, p=2))

# 2-D Tensor 행렬 곱셈  A.matmul(B), A.mm(B), A @ B
행렬 곱을 응용한 대칭 이동 수행
```
